{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0 - Requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\r\n",
    "import torchvision\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import time\r\n",
    "import copy\r\n",
    "from model.ViT_model import ViT_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - OCT dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parameters\r\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
    "batch_size = 128\r\n",
    "input_size = (224,224)\r\n",
    "mean = [.456,.456,.456]\r\n",
    "std = [.224,.224,.224]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load Dataset and Transform\r\n",
    "train_transform = torchvision.transforms.Compose([\r\n",
    "    torchvision.transforms.ToTensor(),\r\n",
    "    torchvision.transforms.RandomResizedCrop(input_size),\r\n",
    "    torchvision.transforms.Normalize((mean), (std))\r\n",
    "])\r\n",
    "\r\n",
    "val_transform = torchvision.transforms.Compose([\r\n",
    "    torchvision.transforms.ToTensor(),\r\n",
    "    torchvision.transforms.CenterCrop(input_size),\r\n",
    "    torchvision.transforms.Normalize((mean), (std))\r\n",
    "])\r\n",
    "\r\n",
    "test_transform = torchvision.transforms.Compose([\r\n",
    "    torchvision.transforms.ToTensor(),\r\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\r\n",
    "    torchvision.transforms.Normalize((mean), (std))\r\n",
    "])\r\n",
    "\r\n",
    "image_dataset = {\r\n",
    "    'train': torchvision.datasets.ImageFolder(os.getcwd() + '/train', train_transform),\r\n",
    "    'val': torchvision.datasets.ImageFolder(os.getcwd() + '/val', val_transform),\r\n",
    "    'test': torchvision.datasets.ImageFolder(os.getcwd() + '/test', test_transform)\r\n",
    "}\r\n",
    "\r\n",
    "data_loader = {\r\n",
    "    x: torch.utils.data.DataLoader(image_dataset[x], batch_size=batch_size,\r\n",
    "                                    shuffle=True, num_workers=4)\r\n",
    "    for x in ['train', 'val', 'test']\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Training pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\r\n",
    "    since = time.time()\r\n",
    "\r\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "    best_acc = 0.0\r\n",
    "\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
    "        print('-' * 10)\r\n",
    "\r\n",
    "        # Each epoch has a training and validation phase\r\n",
    "        for phase in ['train', 'val']:\r\n",
    "            if phase == 'train':\r\n",
    "                model.train()  # Set model to training mode\r\n",
    "            else:\r\n",
    "                model.eval()   # Set model to evaluate mode\r\n",
    "\r\n",
    "            running_loss = 0.0\r\n",
    "            running_corrects = 0\r\n",
    "\r\n",
    "            # Iterate over data.\r\n",
    "            for inputs, labels in dataloaders[phase]:\r\n",
    "                inputs = inputs.to(device)\r\n",
    "                labels = labels.to(device)\r\n",
    "\r\n",
    "                # zero the parameter gradients\r\n",
    "                optimizer.zero_grad()\r\n",
    "\r\n",
    "                # forward\r\n",
    "                # track history if only in train\r\n",
    "                with torch.set_grad_enabled(phase == 'train'):\r\n",
    "                    outputs = model(inputs)\r\n",
    "                    _, preds = torch.max(outputs, 1)\r\n",
    "                    loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "                    # backward + optimize only if in training phase\r\n",
    "                    if phase == 'train':\r\n",
    "                        loss.backward()\r\n",
    "                        optimizer.step()\r\n",
    "\r\n",
    "                # statistics\r\n",
    "                running_loss += loss.item() * inputs.size(0)\r\n",
    "                running_corrects += torch.sum(preds == labels.data)\r\n",
    "            if phase == 'train':\r\n",
    "                scheduler.step()\r\n",
    "\r\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\r\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\r\n",
    "\r\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\r\n",
    "                phase, epoch_loss, epoch_acc))\r\n",
    "\r\n",
    "            # deep copy the model\r\n",
    "            if phase == 'val' and epoch_acc > best_acc:\r\n",
    "                best_acc = epoch_acc\r\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "\r\n",
    "        print()\r\n",
    "\r\n",
    "    time_elapsed = time.time() - since\r\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
    "        time_elapsed // 60, time_elapsed % 60))\r\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\r\n",
    "\r\n",
    "    # load best model weights\r\n",
    "    model.load_state_dict(best_model_wts)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize_model(model, num_images=6):\r\n",
    "    was_training = model.training\r\n",
    "    model.eval()\r\n",
    "    images_so_far = 0\r\n",
    "    fig = plt.figure()\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\r\n",
    "            inputs = inputs.to(device)\r\n",
    "            labels = labels.to(device)\r\n",
    "\r\n",
    "            outputs = model(inputs)\r\n",
    "            _, preds = torch.max(outputs, 1)\r\n",
    "\r\n",
    "            for j in range(inputs.size()[0]):\r\n",
    "                images_so_far += 1\r\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\r\n",
    "                ax.axis('off')\r\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\r\n",
    "                imshow(inputs.cpu().data[j])\r\n",
    "\r\n",
    "                if images_so_far == num_images:\r\n",
    "                    model.train(mode=was_training)\r\n",
    "                    return\r\n",
    "        model.train(mode=was_training)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv_deep_vit_macula': venv)"
  },
  "interpreter": {
   "hash": "aed0aa539dfb232c99d32a3164bc4acb0c41331b0ea40258950c743ac4241d57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}