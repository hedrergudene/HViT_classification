{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0 - Requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from model.tf.ViT_model import ViT_UNet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtenemos en una lista todos los nombres de las imágenes de la carpeta y los parámetros necesarios\r\n",
    "img_path = '/input/'\r\n",
    "img_shape = (128,128)\r\n",
    "batch_size = 32\r\n",
    "seed = 123 #Será necesario para que el método de Keras separe de forma coherente los datos\r\n",
    "class_names = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\r\n",
    "num_classes = len(class_names)\r\n",
    "# Importamos los datos con Keras\r\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
    "  img_path + 'train/',\r\n",
    "  validation_split=0.2,\r\n",
    "  subset='training',\r\n",
    "  color_mode = 'grayscale',\r\n",
    "  shuffle = False,\r\n",
    "  seed=seed,\r\n",
    "  image_size=img_shape,\r\n",
    "  batch_size=batch_size)\r\n",
    "\r\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
    "  img_path + 'val/',\r\n",
    "  validation_split=0.2,\r\n",
    "  subset='validation',\r\n",
    "  color_mode = 'grayscale',\r\n",
    "  shuffle = False,\r\n",
    "  seed=seed,\r\n",
    "  image_size=img_shape,\r\n",
    "  batch_size=batch_size)\r\n",
    "\r\n",
    "# Expresamos las etiquetas en formato ont.hot encoding\r\n",
    "train_data = train_data.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\r\n",
    "val_data = val_data.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Extraemos algo de información de los datos para comprobar que han sido bien importados\r\n",
    "plt.figure(figsize=(10, 10))\r\n",
    "for images, labels in val_data.take(1):\r\n",
    "  for i in range(9):\r\n",
    "    ax = plt.subplot(3, 3, i + 1)\r\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\r\n",
    "    plt.title(class_names[labels[i]])\r\n",
    "    plt.axis(\"off\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Fit model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parámetros\r\n",
    "epochs = 1\r\n",
    "# Instanciamos el nuevo modelo con la GPU habilitada\r\n",
    "input_layer = [tf.keras.Input(shape = (128,128,1))]\r\n",
    "preprocessing_layers = [tf.keras.layers.experimental.preprocessing.Resizing(128,128),\r\n",
    "                        tf.keras.layers.experimental.preprocessing.Rescaling(scale = 1./255, offset = 0.0)]\r\n",
    "if len(tf.config.list_physical_devices('GPU'))>0:\r\n",
    "  print(\"Número de GPUs disponibles: \", len(tf.config.list_physical_devices('GPU')))\r\n",
    "  strategy = tf.distribute.MirroredStrategy()\r\n",
    "  with strategy.scope():\r\n",
    "    model = ViT_UNet(depth = 3,\r\n",
    "                       depth_te = 4,\r\n",
    "                       linear_list = [4],\r\n",
    "                       preprocessing = 'conv',\r\n",
    "                       num_patches = 64,\r\n",
    "                       patch_size = 16,\r\n",
    "                       num_channels = 1,\r\n",
    "                       hidden_dim = 64,\r\n",
    "                       num_heads = 8,\r\n",
    "                       attn_drop = .2,\r\n",
    "                       proj_drop = .2,\r\n",
    "                       linear_drop = .2,\r\n",
    "                       )\r\n",
    "    # Escribimos algunos métodos a aplicar durante el entrenamiento\r\n",
    "    callbacks = [\r\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 2),\r\n",
    "                 tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = .1)\r\n",
    "    ]\r\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n",
    "# Entrenamos modelo \r\n",
    "#model.fit(train_data, epochs=epochs, batch_size = batch_size, validation_data = val_data, callbacks = callbacks, verbose = 1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv_deep_vit_macula': venv)"
  },
  "interpreter": {
   "hash": "aed0aa539dfb232c99d32a3164bc4acb0c41331b0ea40258950c743ac4241d57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}