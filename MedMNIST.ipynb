{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "MedMNIST.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36495608c706448699f009eb73949672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24e173ebb2f44c9bbcc1b5979c3fb0c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_236ffeffe2fd4afda9c0baa206aeb1c7",
              "IPY_MODEL_7863a95b8a964e5ebf1ec229909a7a79"
            ]
          }
        },
        "24e173ebb2f44c9bbcc1b5979c3fb0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "236ffeffe2fd4afda9c0baa206aeb1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_37cb3e8869f1423490a3b6c8636dfeb3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.02MB of 0.02MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18289cd8ac9a479883530fa4e7daa29b"
          }
        },
        "7863a95b8a964e5ebf1ec229909a7a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_967697c07b2946aaa015edadb8cd3935",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5ddb871bfff4080bb16defe806d86a1"
          }
        },
        "37cb3e8869f1423490a3b6c8636dfeb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18289cd8ac9a479883530fa4e7daa29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "967697c07b2946aaa015edadb8cd3935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5ddb871bfff4080bb16defe806d86a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hedrergudene/HViT_classification/blob/main/MedMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8GZS00OwNek"
      },
      "source": [
        "# 0 - Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64namXNWK98v",
        "outputId": "751e4cb3-d752-44f6-acd4-2b0b2735e942"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir macula && unzip /content/drive/MyDrive/archive.zip -d /content/macula >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNMPWiZzwNeq",
        "outputId": "d7911356-8ede-4cdc-b742-eb68cac94140"
      },
      "source": [
        "!git clone https://benayas1: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@github.com/hedrergudene/HViT_classification.git\n",
        "!(cd /content/HViT_classification/ && python setup.py bdist_wheel && pip install dist/hvit-0.0.1-py3-none-any.whl) >> /dev/null\n",
        "!pip install -U tensorflow-addons >> /dev/null\n",
        "!pip install wandb >> /dev/null\n",
        "!pip install ptflops >> /dev/null\n",
        "!pip install timm >> /dev/null\n",
        "!pip install benatools >> /dev/null\n",
        "\n",
        "#!git clone https://github.com/MonashAI/HVT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HViT_classification'...\n",
            "remote: Enumerating objects: 1040, done.\u001b[K\n",
            "remote: Counting objects: 100% (1040/1040), done.\u001b[K\n",
            "remote: Compressing objects: 100% (790/790), done.\u001b[K\n",
            "remote: Total 1040 (delta 567), reused 514 (delta 197), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1040/1040), 1.06 MiB | 4.30 MiB/s, done.\n",
            "Resolving deltas: 100% (567/567), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrU3JElRwNes",
        "outputId": "61594044-5e6b-49a2-d23a-a2349c895263"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from typing import List, Dict\n",
        "import wandb\n",
        "# Import model\n",
        "from hvit.tf.ViT_model import HViT, ViT\n",
        "#from hvit.tf.train_medmnist import run_WB_experiment\n",
        "from hvit.tf.info import INFO\n",
        "from hvit.tf.evaluator import Evaluator\n",
        "import hvit.tf.dataset_without_pytorch as mdn\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Login into W&B\n",
        "WB_ENTITY = 'ual'\n",
        "WB_PROJECT = 'hvit_benchmark'\n",
        "WB_KEY = 'ab1f4c380e0a008223b6434a42907bacfd7b4e26'\n",
        "#WB_KEY = '1bb44e6be47564584868ec55bac8cf468cf0e47f'  # antonio's\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvv77FcaZrOZ"
      },
      "source": [
        "# 1 - Training loop function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyglL7QaZp4O"
      },
      "source": [
        "def load_data(dataclass, split, task, size, n_classes, n_channels):\n",
        "    dataset = dataclass(split=split, download=True)\n",
        "    x = dataset.imgs\n",
        "    if size is not None:\n",
        "        x = np.stack([cv2.resize(img, (size,size), interpolation = cv2.INTER_AREA) for img in x])\n",
        "    if n_channels == 1:\n",
        "        #x = np.expand_dims(x, 3)\n",
        "        x = np.stack([x,x,x], axis=-1)\n",
        "    y = dataset.labels\n",
        "    if task == 'multi-class':\n",
        "        y = tf.keras.utils.to_categorical(y, n_classes)\n",
        "    if task == 'binary-class':\n",
        "        y = np.squeeze(y, axis=1)\n",
        "    return x, y\n",
        "\n",
        "def run_WB_experiment(WB_KEY:str,\n",
        "                      WB_ENTITY:str,\n",
        "                      WB_PROJECT:str,\n",
        "                      WB_GROUP:str,\n",
        "                      model:tf.keras.Model,\n",
        "                      data_flag:str,\n",
        "                      ImageDataGenerator_config:Dict,\n",
        "                      flow_config:Dict,\n",
        "                      epochs:int=10,\n",
        "                      learning_rate:float=0.00005,\n",
        "                      weight_decay:float=0.0001,\n",
        "                      label_smoothing:float=.1,\n",
        "                      es_patience:int=10,\n",
        "                      verbose:int=1,\n",
        "                      resize:int = None,\n",
        "                      ):\n",
        "    # Check for GPU:\n",
        "    assert len(tf.config.list_physical_devices('GPU'))>0, f\"No GPU available. Check system settings.\"\n",
        "\n",
        "    monitor = 'val_AUC'\n",
        "    mode = 'max'\n",
        "\n",
        "    # Generators\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['train'])\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['val'])\n",
        "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**ImageDataGenerator_config['test'])\n",
        "\n",
        "    if data_flag == 'macula':\n",
        "        task = 'multi-class'\n",
        "        n_classes = 4\n",
        "        monitor = 'val_loss'\n",
        "        mode = 'min'\n",
        "        train_generator = train_datagen.flow_from_directory('/content/macula/OCT2017 /train',\n",
        "                                                            target_size=(resize, resize),\n",
        "                                                            color_mode='rgb',\n",
        "                                                            class_mode='categorical',\n",
        "                                                            batch_size=flow_config['train']['batch_size'],\n",
        "                                                            shuffle=flow_config['train']['shuffle'],\n",
        "                                                            seed=flow_config['train']['seed'],\n",
        "                                                            )\n",
        "        val_generator = val_datagen.flow_from_directory('/content/macula/OCT2017 /val',\n",
        "                                                        target_size=(resize, resize),\n",
        "                                                        color_mode='rgb',\n",
        "                                                        class_mode='categorical',\n",
        "                                                        batch_size=flow_config['val']['batch_size'],\n",
        "                                                        shuffle=flow_config['val']['shuffle'],\n",
        "                                                        seed=flow_config['val']['seed'],\n",
        "                                                        )\n",
        "        test_generator = test_datagen.flow_from_directory('/content/macula/OCT2017 /test',\n",
        "                                                          target_size=(resize, resize),\n",
        "                                                          color_mode='rgb',\n",
        "                                                          class_mode='categorical',\n",
        "                                                          batch_size=flow_config['test']['batch_size'],\n",
        "                                                          shuffle=flow_config['test']['shuffle'],\n",
        "                                                          seed=flow_config['test']['seed'],\n",
        "                                                          )\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "            x_val, y_val = x_test, y_test\n",
        "\n",
        "        else:\n",
        "            # Download dataset\n",
        "            info = INFO[data_flag]\n",
        "            task = info['task']\n",
        "            n_channels = info['n_channels']\n",
        "            n_classes = len(info['label'])\n",
        "            n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "            DataClass = getattr(mdn, info['python_class'])\n",
        "            print(f'Dataset {data_flag} Task {task} n_channels {n_channels} n_classes {n_classes}')\n",
        "\n",
        "            # load train Data\n",
        "            x_train, y_train = load_data(DataClass, 'train', task, resize, n_classes, n_channels)\n",
        "\n",
        "            # load val Data\n",
        "            x_val, y_val = load_data(DataClass, 'val', task, resize, n_classes, n_channels)\n",
        "\n",
        "            # load test Data\n",
        "            x_test, y_test = load_data(DataClass, 'test', task, resize, n_classes, n_channels)\n",
        "\n",
        "            print(f'X train {x_train.shape} | Y train {y_train.shape}')\n",
        "            print(f'X val {x_val.shape} | Y val {y_val.shape}')\n",
        "            print(f'X test {x_test.shape} | Y test {y_test.shape}')\n",
        "          \n",
        "            train_generator = train_datagen.flow(x=x_train, \n",
        "                                                y=y_train,\n",
        "                                                batch_size=flow_config['train']['batch_size'],\n",
        "                                                shuffle=flow_config['train']['shuffle'],\n",
        "                                                seed=flow_config['train']['seed'],\n",
        "                                                )\n",
        "            val_generator = val_datagen.flow(x=x_val,\n",
        "                                            y=y_val,\n",
        "                                            batch_size=flow_config['val']['batch_size'],\n",
        "                                            shuffle=flow_config['val']['shuffle'],\n",
        "                                            seed=flow_config['val']['seed'],\n",
        "                                            )\n",
        "            test_generator = test_datagen.flow(x=x_test,\n",
        "                                              y=y_test,\n",
        "                                              batch_size=flow_config['test']['batch_size'],\n",
        "                                              shuffle=flow_config['test']['shuffle'],\n",
        "                                              seed=flow_config['test']['seed'],\n",
        "                                              )\n",
        "    # Log in WB\n",
        "    wandb.login(key=WB_KEY)\n",
        "\n",
        "    # Train & validation steps\n",
        "    train_steps_per_epoch = len(train_generator)\n",
        "    val_steps_per_epoch = len(val_generator)\n",
        "    test_steps_per_epoch = len(test_generator)\n",
        "\n",
        "    # Save initial weights\n",
        "    #model.load_weights(os.path.join(os.getcwd(), 'model_weights.h5'))\n",
        "\n",
        "    # Credentials\n",
        "    wandb.init(project='_'.join([WB_PROJECT, data_flag]), entity=WB_ENTITY, group = WB_GROUP)\n",
        "    \n",
        "    # Model compile\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if task == 'multi-class':\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing = label_smoothing)\n",
        "        metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "                   tf.keras.metrics.AUC(multi_label=True, num_labels=n_classes, from_logits=True, name=\"AUC\"),\n",
        "                   tfa.metrics.F1Score(num_classes=n_classes, average='macro', name = 'f1_score')\n",
        "                   ]\n",
        "    if task == 'binary-class':\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing = label_smoothing)\n",
        "        metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "                   tf.keras.metrics.AUC(multi_label=False, from_logits=True, name=\"AUC\")]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, mode=mode, factor=0.2, patience=int(es_patience/2), min_lr=learning_rate//100, verbose=1)\n",
        "    patience = tf.keras.callbacks.EarlyStopping(monitor=monitor, mode=mode, patience=es_patience)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(os.getcwd(), 'model_best_weights.h5'), monitor=monitor, mode=mode, save_best_only = True, save_weights_only = True)\n",
        "    wandb_callback = wandb.keras.WandbCallback(save_weights_only=True)\n",
        "\n",
        "    # Model fit\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch= train_steps_per_epoch,\n",
        "        epochs = epochs,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps = val_steps_per_epoch,\n",
        "        callbacks=[reduceLR, patience, checkpoint, wandb_callback],\n",
        "        verbose = verbose,\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    model.load_weights(os.path.join(os.getcwd(), 'model_best_weights.h5'))\n",
        "    results = model.evaluate(test_generator, steps = test_steps_per_epoch, verbose = 0)\n",
        "    print(\"Test metrics:\",{k:v for k,v in zip(model.metrics_names, results)})\n",
        "    wandb.log({(\"test_\"+k):v for k,v in zip(model.metrics_names, results)})\n",
        "    wandb.log({\"n_parameters\":np.round(model.count_params()/1000000, 1)})\n",
        "\n",
        "    #y_pred = model.predict(test_generator, verbose = 0)\n",
        "    #evaluator = Evaluator(data_flag, 'test')\n",
        "    #results = evaluator.evaluate(y_pred)\n",
        "\n",
        "    #print(f\"Test metrics: AUC {results.AUC}, ACC {results.ACC}\")\n",
        "    #wandb.log({\"test_ACC\":results.ACC, \"test_AUC\":results.AUC})\n",
        "\n",
        "    # Clear memory\n",
        "    tf.keras.backend.clear_session()\n",
        "    wandb.finish()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N034mIbwfSY"
      },
      "source": [
        "# 2 - Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP99yr--wnkI"
      },
      "source": [
        "# Config\n",
        "# 'pneumoniamnist','breastmnist'\n",
        "#datasets = ['octmnist','tissuemnist','pathmnist','dermamnist','bloodmnist', 'organamnist', 'organcmnist', 'organsmnist']\n",
        "datasets = ['bloodmnist']\n",
        "#datasets = ['macula']\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 3\n",
        "es_patience = 7\n",
        "seed = 7853\n",
        "verbose=1\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = .1\n",
        "img_size = 32\n",
        "\n",
        "ImageDataGenerator_config = {\n",
        "    'train':{\n",
        "        \"rescale\":1./255,\n",
        "        \"shear_range\":.1,\n",
        "        \"rotation_range\":.2,\n",
        "        \"zoom_range\":.1,\n",
        "        \"horizontal_flip\" : True,\n",
        "        },\n",
        "    'val':{\n",
        "        \"rescale\":1./255,\n",
        "        },\n",
        "    'test':{\n",
        "        \"rescale\":1./255,\n",
        "        }\n",
        "}\n",
        "flow_config = {\n",
        "    'train':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":True,\n",
        "        \"seed\":seed,\n",
        "        },\n",
        "    'val':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":False,\n",
        "        \"seed\":seed,\n",
        "        },\n",
        "    'test':{\n",
        "        \"batch_size\":batch_size,\n",
        "        \"shuffle\":False,\n",
        "        \"seed\":seed,\n",
        "        }\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW71oW4MwNeu"
      },
      "source": [
        "# 3 - Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEfKwXGdwNez"
      },
      "source": [
        "## HViT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OR--_LKwNe0"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "WB_GROUP = 'HViT'\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    hvit_params = { 'img_size':img_size,\n",
        "                    'patch_size':[2,4,8],\n",
        "                    'num_channels': 3,\n",
        "                    'num_heads': 8,\n",
        "                    'transformer_layers':[4,4,4],\n",
        "                    'hidden_unit_factor':2,\n",
        "                    'mlp_head_units': [256, 64],\n",
        "                    'num_classes':n_classes,\n",
        "                    'drop_attn':0.2,\n",
        "                    'drop_proj':0.2,\n",
        "                    'drop_linear':0.4,\n",
        "                    'projection_dim' : 48,\n",
        "                    'resampling_type':\"conv\",\n",
        "                    'original_attn':True,\n",
        "                    }\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "            hvit_params = { 'img_size':128,\n",
        "                    'patch_size':[8,16,32],\n",
        "                    'num_channels': 3,\n",
        "                    'num_heads': 8,\n",
        "                    'transformer_layers':[4,4,4],\n",
        "                    'hidden_unit_factor':2,\n",
        "                    'mlp_head_units': [256, 64],\n",
        "                    'num_classes':n_classes,\n",
        "                    'drop_attn':0.2,\n",
        "                    'drop_proj':0.2,\n",
        "                    'drop_linear':0.4,\n",
        "                    'projection_dim' : 768,\n",
        "                    'resampling_type':\"conv\",\n",
        "                    'original_attn':True,\n",
        "                    }\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      outputs = HViT(**hvit_params)(inputs)\n",
        "      model = tf.keras.Model(inputs, outputs)\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmj5bHpUsP1H"
      },
      "source": [
        "## ViT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gto-rgFwswDR"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'ViT'\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "        vit_params = {'img_size':img_size,\n",
        "                  'patch_size':4,\n",
        "                  'num_channels': 3,\n",
        "                  'num_heads': 8,\n",
        "                  'transformer_layers':16,\n",
        "                  'hidden_unit_factor':4,\n",
        "                  'mlp_head_units': [256, 64],\n",
        "                  'num_classes':n_classes,\n",
        "                  'drop_attn':0.2,\n",
        "                  'drop_proj':0.2,\n",
        "                  'drop_linear':0.4,\n",
        "                  'projection_dim' : 3*16\n",
        "                  }\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "            vit_params = {'img_size':img_size,\n",
        "                      'patch_size':4,\n",
        "                      'num_channels': 3,\n",
        "                      'num_heads': 8,\n",
        "                      'transformer_layers':16,\n",
        "                      'hidden_unit_factor':4,\n",
        "                      'mlp_head_units': [256, 64],\n",
        "                      'num_classes':n_classes,\n",
        "                      'drop_attn':0.2,\n",
        "                      'drop_proj':0.2,\n",
        "                      'drop_linear':0.4,\n",
        "                      'projection_dim' : 3*16\n",
        "                      }\n",
        "        else:\n",
        "            n_classes = 4\n",
        "            vit_params = {'img_size':img_size,  # 128\n",
        "                          'patch_size':16,\n",
        "                          'num_channels': 3,\n",
        "                          'num_heads': 8,\n",
        "                          'transformer_layers':12,\n",
        "                          'hidden_unit_factor':2,\n",
        "                          'mlp_head_units': [256, 64],\n",
        "                          'num_classes':n_classes,\n",
        "                          'drop_attn':0.2,\n",
        "                          'drop_proj':0.2,\n",
        "                          'drop_linear':0.4,\n",
        "                          'projection_dim' : 768,\n",
        "                          'resampling_type':\"conv\",\n",
        "                          'original_attn':True,\n",
        "                          }\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        # Instance model\n",
        "        inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "        outputs = ViT(**vit_params)(inputs)\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        # Run experiment\n",
        "        run_WB_experiment(WB_KEY,\n",
        "                          WB_ENTITY,\n",
        "                          WB_PROJECT,\n",
        "                          WB_GROUP,\n",
        "                          model,\n",
        "                          data_flag,\n",
        "                          ImageDataGenerator_config,\n",
        "                          flow_config,\n",
        "                          epochs=epochs,\n",
        "                          learning_rate=learning_rate,\n",
        "                          weight_decay=weight_decay,\n",
        "                          label_smoothing = label_smoothing,\n",
        "                          verbose=verbose,\n",
        "                          resize=img_size,\n",
        "                          es_patience=es_patience,\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WIq-kkGzCNZ"
      },
      "source": [
        "## EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQFWFM8tzBc7"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'EfficientNetB0'\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.EfficientNetB0(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUnseswN_jZe"
      },
      "source": [
        "## EfficientNetB4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6To5LCO0_ieW"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = 'EfficientNetB4'\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.EfficientNetB4(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMgBP9xY2dus"
      },
      "source": [
        "## ResNet 150v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7jBSm952mDN"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"ResNet 152 v2\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      base_model = tf.keras.applications.resnet_v2.ResNet152V2(weights=None, include_top=False)(inputs)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PT7AGvWvYc"
      },
      "source": [
        "## Conv Mixer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjBIdeetWxme"
      },
      "source": [
        "def activation_block(x, dropout=.2):\n",
        "    x = tf.keras.layers.Activation(\"gelu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int, dropout: float):\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x, dropout)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int, dropout: float):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.Add()([activation_block(x, dropout), x0])  # Residual.\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x, dropout)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=32, filters=256, depth=12, kernel_size=5, patch_size=4, mlp_head_units:List[int]=[256,64], drop_enc:float=.2, drop_linear:float=.2, num_classes=10,\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input((image_size, image_size, 3))\n",
        "    x = tf.keras.layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size, drop_enc)\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size, drop_enc)\n",
        "\n",
        "    # Classification block.\n",
        "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "    for i in mlp_head_units:\n",
        "        x = tf.keras.layers.Dense(i)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "    logits = tf.keras.layers.Dense(num_classes)(x)\n",
        "    return tf.keras.Model(inputs, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVXlsCiW03f"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"ConvMixer\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      model = get_conv_mixer_256_8(patch_size = 4, num_classes = n_classes)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDFy4Af3g_P"
      },
      "source": [
        "## Inception ResNet v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnIY2iv3mXI"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"Inception ResNet v2\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "img_size=128\n",
        "for data_flag in datasets:\n",
        "\n",
        "    info = INFO[data_flag]\n",
        "    n_classes = len(info['label'])\n",
        "    n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "    # Start running\n",
        "    with tf.device('/device:GPU:0'):\n",
        "\n",
        "      # Instance model\n",
        "      inputs = tf.keras.layers.Input((img_size, img_size, 3))\n",
        "      x = tf.keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
        "      base_model = tf.keras.applications.InceptionResNetV2(weights=None, include_top=False)(x)\n",
        "      x = tf.keras.layers.GlobalAveragePooling2D()(base_model)\n",
        "      for i in mlp_head_units:\n",
        "          x = tf.keras.layers.Dense(i)(x)\n",
        "          x = tf.keras.layers.Dropout(drop_linear)(x)\n",
        "      logits = tf.keras.layers.Dense(n_classes)(x)\n",
        "      model = tf.keras.Model(inputs, logits)\n",
        "\n",
        "      # Run experiment\n",
        "      run_WB_experiment(WB_KEY,\n",
        "                        WB_ENTITY,\n",
        "                        WB_PROJECT,\n",
        "                        WB_GROUP,\n",
        "                        model,\n",
        "                        data_flag,\n",
        "                        ImageDataGenerator_config,\n",
        "                        flow_config,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=weight_decay,\n",
        "                        label_smoothing = label_smoothing,\n",
        "                        verbose=verbose,\n",
        "                        resize=img_size,\n",
        "                        es_patience=es_patience,\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6njEsl4MDE"
      },
      "source": [
        "## HVT (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxnlpz8xyDkf"
      },
      "source": [
        "import torch\n",
        "from benatools.torch.fitter import TorchFitterBase\n",
        "from hvit.pytorch.HVT.models import hvt_model\n",
        "from hvit.pytorch.HVT.params import args\n",
        "from hvit.pytorch.HVT.datasets import build_transform\n",
        "from torchvision import transforms\n",
        "\n",
        "import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from hvit.pytorch.HVT.models import hvt_model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "args['input-size'] = img_size\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, label, augments=None, im_size=32):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.augments = augments\n",
        "        self.im_size = im_size\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        x = cv2.resize(self.data[idx], (self.im_size, self.im_size))\n",
        "        y = self.label[idx]\n",
        "        \n",
        "        # Augmentation including scaling\n",
        "        if self.augments:\n",
        "            x = self.toTensor(x)\n",
        "            x = self.augments(x)\n",
        "\n",
        "        return {'x':x, 'y':y}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class ImageFitter(TorchFitterBase):\n",
        "    def unpack(self, data):\n",
        "        # extract x and y from the dataloader\n",
        "        x = data['x'].to(self.device).float()\n",
        "        y = data['y'].to(self.device).float()\n",
        "\n",
        "        # weights if existing\n",
        "        return x, y, None\n",
        "\n",
        "def get_transform(is_train, size):\n",
        "    transforms.Compose([\n",
        "        transforms.RandomAffine(.2, translate=None, scale=1./255, shear=.1, fill=0, fillcolor=None, resample=None),\n",
        "        \"rescale\":1./255,\n",
        "        \"shear_range\":.1,\n",
        "        \"rotation_range\":.2,\n",
        "        \"zoom_range\":.1,\n",
        "        \"horizontal_flip\" : True, \n",
        "    ]\n",
        "        \n",
        "    )\n",
        "\n",
        "def get_data(data_flag, resize=32):\n",
        "    # Download dataset\n",
        "    info = INFO[data_flag]\n",
        "    task = info['task']\n",
        "    n_channels = info['n_channels']\n",
        "    n_classes = len(info['label'])\n",
        "    n_classes = 1 if n_classes == 2 else n_classes\n",
        "\n",
        "    DataClass = getattr(mdn, info['python_class'])\n",
        "    print(f'Dataset {data_flag} Task {task} n_channels {n_channels} n_classes {n_classes}')\n",
        "\n",
        "    # load train Data\n",
        "    x_train, y_train = load_data(DataClass, 'train', task, resize, n_classes, n_channels)\n",
        "\n",
        "    # load val Data\n",
        "    x_val, y_val = load_data(DataClass, 'val', task, resize, n_classes, n_channels)\n",
        "\n",
        "    # load test Data\n",
        "    x_test, y_test = load_data(DataClass, 'test', task, resize, n_classes, n_channels)\n",
        "\n",
        "    print(f'X train {x_train.shape} | Y train {y_train.shape}')\n",
        "    print(f'X val {x_val.shape} | Y val {y_val.shape}')\n",
        "    print(f'X test {x_test.shape} | Y test {y_test.shape}')\n",
        "\n",
        "          \n",
        "    train_generator = ImageDataset(x_train, y_train, build_transform(is_train=True, args=args), resize)\n",
        "    val_generator = ImageDataset(x_val, y_val, build_transform(is_train=False, args=args), resize)\n",
        "    test_generator = ImageDataset(x_test, y_test, build_transform(is_train=False, args=args), resize)\n",
        "\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_generator,\n",
        "                                                   batch_size=flow_config['train']['batch_size'],\n",
        "                                                   shuffle=flow_config['train']['shuffle'],\n",
        "                                                   num_workers=2\n",
        "                                                   )\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_generator,\n",
        "                                                 batch_size=flow_config['val']['batch_size'],\n",
        "                                                 shuffle=flow_config['val']['shuffle'],\n",
        "                                                 num_workers=2,\n",
        "                                                 )\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_generator,\n",
        "                                                  batch_size=flow_config['test']['batch_size'],\n",
        "                                                  shuffle=flow_config['test']['shuffle'],\n",
        "                                                  num_workers=2,\n",
        "                                                  )\n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "36495608c706448699f009eb73949672",
            "24e173ebb2f44c9bbcc1b5979c3fb0c0",
            "236ffeffe2fd4afda9c0baa206aeb1c7",
            "7863a95b8a964e5ebf1ec229909a7a79",
            "37cb3e8869f1423490a3b6c8636dfeb3",
            "18289cd8ac9a479883530fa4e7daa29b",
            "967697c07b2946aaa015edadb8cd3935",
            "e5ddb871bfff4080bb16defe806d86a1"
          ]
        },
        "id": "ltt4DKk1qkEa",
        "outputId": "48188696-0513-4243-bed4-fe4af1d0c9cf"
      },
      "source": [
        "def wandb_update(x):\n",
        "    data_log = x.copy()\n",
        "    del data_log['epoch']\n",
        "    wandb.log({'training':data_log})\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "WB_GROUP = \"HVT-small\"\n",
        "mlp_head_units = [256,64]\n",
        "drop_linear = .2\n",
        "\n",
        "if WB_GROUP == \"HVT-small\":\n",
        "    model_params = { #small\n",
        "      \"model\": \"hvt_model\",\n",
        "      \"batch_size\": batch_size,\n",
        "      \"exp_name\": \"hvt-s-1\",\n",
        "      \"input_size\": img_size, #224,\n",
        "      \"patch_size\": 16,\n",
        "      \"num_heads\": 6,\n",
        "      \"head_dim\": 64,\n",
        "      \"num_blocks\": 12,\n",
        "      \"num_workers\": 10,\n",
        "      \"pool_kernel_size\": 3,\n",
        "      \"pool_stride\": 2,\n",
        "      \"pool_block_width\": 12,\n",
        "      \"weight_decay\": 0.025\n",
        "    }\n",
        "else:\n",
        "    model_params = {\n",
        "      \"model\": \"hvt_model\",\n",
        "      \"batch_size\": batch_size,\n",
        "      \"exp_name\": \"hvt-ti-1\",\n",
        "      \"input_size\": img_size, #224,\n",
        "      \"patch_size\": 16,\n",
        "      \"num_heads\": 3,\n",
        "      \"head_dim\": 64,\n",
        "      \"num_blocks\": 12,\n",
        "      \"num_workers\": 10,\n",
        "      \"pool_kernel_size\": 3,\n",
        "      \"pool_stride\": 2,\n",
        "      \"pool_block_width\": 12,\n",
        "      \"weight_decay\": 0.025\n",
        "    }\n",
        "\n",
        "device = torch.device(args['device'])\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "for data_flag in datasets:\n",
        "\n",
        "    if data_flag in INFO:\n",
        "        info = INFO[data_flag]\n",
        "        n_classes = len(info['label'])\n",
        "        n_classes = 1 if n_classes == 2 else n_classes\n",
        "    else:\n",
        "        if data_flag == 'cifar100':\n",
        "            n_classes = 100\n",
        "        else:\n",
        "            n_classes = 4\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_data(data_flag, img_size)\n",
        "\n",
        "    wandb.login(key=WB_KEY)\n",
        "    wandb.init(project='_'.join([WB_PROJECT, data_flag]), entity=WB_ENTITY, group = WB_GROUP)\n",
        "\n",
        "    model = hvt_model(\n",
        "        pretrained=False,\n",
        "        head_dim=model_params['head_dim'],\n",
        "        num_heads=model_params['num_heads'],\n",
        "        input_size=model_params['input_size'],\n",
        "        patch_size=model_params['patch_size'],\n",
        "        num_blocks=model_params['num_blocks'],\n",
        "        pool_block_width=model_params['pool_block_width'],\n",
        "        pool_kernel_size=model_params['pool_kernel_size'],\n",
        "        num_classes=n_classes,\n",
        "        drop_rate=drop_linear,\n",
        "        drop_path_rate=0.1,\n",
        "    )\n",
        "\n",
        "    print(str(model))\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params: ' + str(n_parameters))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=int(es_patience/2), min_lr=learning_rate//100, verbose=1)\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=label_smoothing)\n",
        " \n",
        "    fitter = ImageFitter(model,\n",
        "                         loss=criterion,\n",
        "                         optimizer=optimizer,\n",
        "                         scheduler = lr_scheduler,\n",
        "                         device=device,\n",
        "                         folder='models_HVT',\n",
        "                         use_amp=False)\n",
        "    \n",
        "    history = fitter.fit(train_loader,\n",
        "                         val_loader=val_loader,\n",
        "                         n_epochs=epochs, \n",
        "                         early_stopping=es_patience,\n",
        "                         early_stopping_mode='max',\n",
        "                         metrics = [(f1_score, {'average':'macro'}), (accuracy_score, {}), (roc_auc_score, {})], \n",
        "                         save_checkpoint=False,\n",
        "                         save_best_checkpoint=True,\n",
        "                         verbose_steps=5,\n",
        "                         callbacks=[wandb_update])\n",
        "    \n",
        "    fitter.load('models_HVT/best-checkpoint.bin')\n",
        "    \n",
        "    predictions = fitter.predict(test_loader)\n",
        "\n",
        "    wandb.finish()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset bloodmnist Task multi-class n_channels 3 n_classes 8\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train (11959, 32, 32, 3) | Y train (11959, 8)\n",
            "X val (1712, 32, 32, 3) | Y val (1712, 8)\n",
            "X test (3421, 32, 32, 3) | Y test (3421, 8)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:3ean32ge) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 343... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36495608c706448699f009eb73949672",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">olive-eon-38</strong>: <a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/3ean32ge\" target=\"_blank\">https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/3ean32ge</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211114_221825-3ean32ge/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:3ean32ge). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist/runs/38nz3as1\" target=\"_blank\">ruby-tree-39</a></strong> to <a href=\"https://wandb.ai/ual/hvit_benchmark_bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HVT(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (downsample): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (drop_path): DropPath()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  (head): Linear(in_features=384, out_features=8, bias=True)\n",
            ")\n",
            "number of params: 21594632\n",
            "Fitter prepared. Device is cuda\n",
            "\n",
            "2021-11-14 22:20:12\n",
            "                         EPOCH 1/3 - LR: 0.0001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4fe1b081b30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m                          \u001b[0msave_best_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                          \u001b[0mverbose_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                          callbacks=[wandb_update])\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mfitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models_HVT/best-checkpoint.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/benatools/torch/fitter.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, val_loader, n_epochs, metrics, early_stopping, early_stopping_mode, early_stopping_alpha, early_stopping_pct, save_checkpoint, save_best_checkpoint, verbose_steps, callbacks)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# Run one training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mtrain_summary_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_summary_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m  \u001b[0;31m# training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/benatools/torch/fitter.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader, verbose_steps)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# run epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mverbose_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-5f21794b3639>\", line 43, in __getitem__\n    x = self.augments(x)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.7/dist-packages/timm/data/transforms.py\", line 140, in __call__\n    i, j, h, w = self.get_params(img, self.scale, self.ratio)\n  File \"/usr/local/lib/python3.7/dist-packages/timm/data/transforms.py\", line 102, in get_params\n    area = img.size[0] * img.size[1]\nTypeError: 'builtin_function_or_method' object is not subscriptable\n"
          ]
        }
      ]
    }
  ]
}